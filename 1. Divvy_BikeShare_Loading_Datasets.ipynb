{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns lists were created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Prepare lists\n",
    "to_delete_cols   = ['trip_id', 'windchill', 'precipitation', 'dewpoint', 'from_station_id', 'to_station_id']\n",
    "to_datetime_cols = ['starttime','stoptime']\n",
    "to_category_cols = ['from_station_name','to_station_name','events','conditions']\n",
    "to_round4_latlon = ['latitude_start', 'longitude_start', 'latitude_end', 'longitude_end']\n",
    "to_int8_cols     = ['usertype', 'gender', 'dpcapacity_start', 'dpcapacity_end', 'tripduration', 'humidity', 'rain', 'dpcapacity_start', 'dpcapacity_end', 'temperature', 'pressure', 'visibility', 'wind_speed']\n",
    "\n",
    "print('Columns lists were created\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_process_datasets(path, to_del, to_datetime, to_int, to_cat, lat_lon):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Store strating time\n",
    "        process_start = datetime.now()\n",
    "\n",
    "        print('Processing {} ...\\n'.format(path))\n",
    "\n",
    "        # Load dataset into chuncks --------------------------------------------------------------------------------------------------------------------\n",
    "        chunck_reader = pd.read_csv(path, chunksize = 500000)\n",
    "\n",
    "        df_list = []\n",
    "        i = 0\n",
    "\n",
    "        for df in chunck_reader:\n",
    "            df_list.append(df) # Append every chunk into df_list \n",
    "            i = i + 1\n",
    "            print('Chunk: {} - length = {} - is processed and appended'.format(i, len(df)))\n",
    "\n",
    "        print('\\nAll chunks were processed and appended')\n",
    "        print('All datasets loaded successfully\\n\\n')\n",
    "\n",
    "        # Concat nested lists --------------------------------------------------------------------------------------------------------------------------\n",
    "        main_df = pd.concat(df_list, sort = False)\n",
    "        main_df.reset_index(drop = True, inplace = True)\n",
    "        print('Main dataframe is created by concatenating all chunks')\n",
    "\n",
    "        # Delete unwanted object to better memory usage\n",
    "        del chunck_reader, df, df_list\n",
    "        print('Release objects for better memory usage\\n\\n')\n",
    "\n",
    "        # Store the length of the dataframe\n",
    "        orig_df = len(main_df)  # original dataset length\n",
    "        print('Dataset length is: {}'.format(orig_df))\n",
    "\n",
    "        # Memory usage before optimization\n",
    "        bo = round(main_df.memory_usage(deep = True).sum() / 1000**3, 2)   # Convert bytes to GBs\n",
    "        print('DataFrame memory usage before optimization: {} GB\\n\\n'.format(bo))\n",
    "\n",
    "        # Check numeric features missing values ---------------------------------------------------------------------------------------------------------\n",
    "        main_df.loc[main_df.gender == -25, 'gender'] = np.nan\n",
    "        main_df.loc[main_df.dpcapacity_start == -25, 'dpcapacity_start'] = np.nan\n",
    "        main_df.loc[main_df.dpcapacity_end == -25, 'dpcapacity_end'] = np.nan\n",
    "        main_df.loc[main_df.windchill == -999.0, 'windchill'] = np.nan\n",
    "        main_df.loc[main_df.precipitation == -9999.0, 'precipitation'] = np.nan\n",
    "        main_df.loc[main_df.wind_speed == -9999.0, 'wind_speed'] = np.nan\n",
    "        main_df.loc[main_df.visibility == -9999.0, 'visibility'] = np.nan\n",
    "        main_df.loc[main_df.pressure == -9999.0, 'pressure'] = np.nan\n",
    "        main_df.loc[main_df.temperature == -9999.000000, 'temperature'] = np.nan\n",
    "        main_df.loc[main_df.dewpoint == -9999.0, 'dewpoint'] = np.nan\n",
    "\n",
    "        # Check categorical features missing values\n",
    "        main_df.loc[main_df.events == 'unknown', 'events'] = np.nan\n",
    "        main_df.loc[main_df.conditions == 'unknown', 'conditions'] = np.nan\n",
    "\n",
    "        # Create list of the columns to test ------------------------------------------------------------------------------------------------------------\n",
    "        numeric_cols = ['gender', 'dpcapacity_end', 'windchill', 'wind_speed', 'precipitation', 'visibility', 'pressure', 'temperature', 'dewpoint', 'events', 'conditions']\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            missing_val = main_df[col].isnull().sum()\n",
    "            print('{} feature contains: {} missing values. {}% of the dataset'.format(col, missing_val, round((missing_val / orig_df) * 100, 2)))\n",
    "\n",
    "        # Drop unwanted columns -------------------------------------------------------------------------------------------------------------------------\n",
    "        main_df.drop(to_del, axis = 1, inplace = True)\n",
    "        print('\\n{} features were droppped successfully\\n\\n'.format(to_del))\n",
    "\n",
    "        # latitude_start: Assigning the 'Clark St & 9th St (AMLI)' LAT & LON for the missing values in latitude_start & longitude_start features ------------------------\n",
    "        main_df.loc[main_df.latitude_start.isnull(), 'latitude_start'] = 41.870815\n",
    "        main_df.loc[main_df.longitude_start.isnull(), 'longitude_start'] = -87.631248\n",
    "        print('Missing start LAT & LON fixed')\n",
    "\n",
    "        # latitude_end: Assigning the 'Clark St & 9th St (AMLI)' LAT & LON for the missing values in latitude_start & longitude_start features ------------------------\n",
    "        main_df.loc[main_df.latitude_end.isnull(), 'latitude_end'] = 41.870815\n",
    "        main_df.loc[main_df.longitude_end.isnull(), 'longitude_end'] = -87.631248\n",
    "        print('Missing end LAT & LON fixed\\n\\n')\n",
    "\n",
    "        # Fill nan values for specific features using the method = 'ffill'\n",
    "        #df.loc[:,['temperature', 'pressure', 'pressure']].fillna(method= 'ffill', inplace=True)\n",
    "        #print('['temperature', 'pressure', 'pressure'] features were treated\\n\\n')\n",
    "\n",
    "        # Drop rows where they contain Nan values and reset the indexes\n",
    "        main_df.dropna(inplace = True)\n",
    "        main_df.reset_index(drop = True, inplace = True)\n",
    "        print('Null values were dropped and indexes were reset successfully\\n\\n')\n",
    "\n",
    "        # Encode classes ------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        main_df.gender.replace({'Male': 0, 'Female': '1'}, inplace= True) \n",
    "        print('Gender feature encoded')\n",
    "\n",
    "        # Process usertype encode classes\n",
    "        main_df.usertype.replace({'Subscriber': 0, 'Customer': 1, 'Dependent': 3}, inplace= True)\n",
    "        print('UserType feature encoded\\n\\n')\n",
    "\n",
    "        # Converting datatypes ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        print('Processing Datatype conversion ...\\n')\n",
    "\n",
    "        # Process int8 columns\n",
    "        for col in to_int:\n",
    "            main_df[col] = main_df[col].astype('int8')\n",
    "            print('{} column processed...'.format(col))\n",
    "        print('Int columns processed successfully\\n')\n",
    "\n",
    "        # Process category columns\n",
    "        for col in to_cat:\n",
    "            main_df[col] = main_df[col].astype('category')\n",
    "            print('{} column processed...'.format(col))\n",
    "        print('Category columns processed successfully\\n')\n",
    "\n",
    "        # Print note\n",
    "        print('Processing Datetime features ...\\n')    \n",
    "\n",
    "        # Process datetime columns\n",
    "        for col in to_datetime:\n",
    "            main_df[col] = pd.to_datetime(main_df[col])\n",
    "            print('{} column processed...'.format(col))\n",
    "        print('Datetime columns processed successfully\\n\\n')\n",
    "        \n",
    "        # Process LAT & LON columns and convert them to float32\n",
    "        for col in to_round4_latlon:\n",
    "            main_df[col] = main_df[col].apply(lambda x: round(x,3))\n",
    "            print('{} column processed...'.format(col))\n",
    "        print('4 decimal points LAT & LON processed successfully\\n')\n",
    "\n",
    "        # Create new trip duration feature ------------------------------------------------------------------------------------------------------------------------------\n",
    "        main_df['new_tripduration'] = main_df.stoptime - main_df.starttime\n",
    "        main_df.new_tripduration = main_df.new_tripduration.astype('timedelta64[s]') # To convert 00:05:00 datetime to seconds\n",
    "        main_df.new_tripduration = main_df.new_tripduration.astype('int16') # int8 will convert numbers and give negatove values, instead will use int16\n",
    "        main_df.drop('tripduration', axis = 1, inplace= True) # Drop column\n",
    "        print('[new_tripduration] feature created based on (stoptime - starttime) and the [tripduration] dropped')\n",
    "\n",
    "        # Remove records where trip duration is less than 300 seconds - (1,268,968 Rows)\n",
    "        main_df = main_df[(main_df.new_tripduration >= 300) & (main_df.new_tripduration <= 3600)] # Keep the data where trip duration is between 5 to 60 mins only.\n",
    "        main_df.reset_index(drop= True, inplace= True)\n",
    "        print('Data with trip duration between 5 to 60 mins kept\\n\\n')\n",
    "        \n",
    "        # Remove 2013 and 2014 from the dataset \n",
    "        main_df = main_df.loc[main_df.starttime.dt.year > 2014]\n",
    "        main_df.sort_values(by = 'starttime', inplace = True)\n",
    "        main_df.reset_index(drop = True, inplace = True)\n",
    "        print('2013 and 2014 records dropped')\n",
    "        print('New dataset length is: {} records. {} records dropped\\n\\n'.format(len(main_df), orig_df - len(main_df)))\n",
    "\n",
    "        # Percentage of removed values ----------------------------------------------------------------------------------------------------------------------------------\n",
    "        print('Dropped values percentage: {}%'.format(round(((orig_df - len(main_df)) / orig_df) * 100, 1)))\n",
    "\n",
    "        # Memory usage before optimization ------------------------------------------------------------------------------------------------------------------------------\n",
    "        ao = round(main_df.memory_usage(deep = True).sum() / 1000**3, 2)   # Convert bytes to GBs\n",
    "        print('DataFrame memory usage after optimization: {} GB\\n\\n'.format(ao))\n",
    "\n",
    "        # Store ending time\n",
    "        process_end = datetime.now()\n",
    "\n",
    "        # Finish statement\n",
    "        print('Processing time: {}'.format(process_end - process_start))\n",
    "\n",
    "        return main_df\n",
    "\n",
    "    except:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset/data_raw.csv ...\n",
      "\n",
      "Chunk: 1 - length = 500000 - is processed and appended\n",
      "Chunk: 2 - length = 500000 - is processed and appended\n",
      "Chunk: 3 - length = 500000 - is processed and appended\n",
      "Chunk: 4 - length = 500000 - is processed and appended\n",
      "Chunk: 5 - length = 500000 - is processed and appended\n",
      "Chunk: 6 - length = 500000 - is processed and appended\n",
      "Chunk: 7 - length = 500000 - is processed and appended\n",
      "Chunk: 8 - length = 500000 - is processed and appended\n",
      "Chunk: 9 - length = 500000 - is processed and appended\n",
      "Chunk: 10 - length = 500000 - is processed and appended\n",
      "Chunk: 11 - length = 500000 - is processed and appended\n",
      "Chunk: 12 - length = 500000 - is processed and appended\n",
      "Chunk: 13 - length = 500000 - is processed and appended\n",
      "Chunk: 14 - length = 500000 - is processed and appended\n",
      "Chunk: 15 - length = 500000 - is processed and appended\n",
      "Chunk: 16 - length = 500000 - is processed and appended\n",
      "Chunk: 17 - length = 500000 - is processed and appended\n",
      "Chunk: 18 - length = 500000 - is processed and appended\n",
      "Chunk: 19 - length = 500000 - is processed and appended\n",
      "Chunk: 20 - length = 500000 - is processed and appended\n",
      "Chunk: 21 - length = 500000 - is processed and appended\n",
      "Chunk: 22 - length = 500000 - is processed and appended\n",
      "Chunk: 23 - length = 500000 - is processed and appended\n",
      "Chunk: 24 - length = 500000 - is processed and appended\n",
      "Chunk: 25 - length = 500000 - is processed and appended\n",
      "Chunk: 26 - length = 500000 - is processed and appended\n",
      "Chunk: 27 - length = 500000 - is processed and appended\n",
      "Chunk: 28 - length = 274715 - is processed and appended\n",
      "\n",
      "All chunks were processed and appended\n",
      "All datasets loaded successfully\n",
      "\n",
      "\n",
      "Main dataframe is created by concatenating all chunks\n",
      "Release objects for better memory usage\n",
      "\n",
      "\n",
      "Dataset length is: 13774715\n",
      "DataFrame memory usage before optimization: 9.93 GB\n",
      "\n",
      "\n",
      "gender feature contains: 3756932 missing values. 27.27% of the dataset\n",
      "dpcapacity_end feature contains: 1180 missing values. 0.01% of the dataset\n",
      "windchill feature contains: 11837251 missing values. 85.93% of the dataset\n",
      "wind_speed feature contains: 4253 missing values. 0.03% of the dataset\n",
      "precipitation feature contains: 12833368 missing values. 93.17% of the dataset\n",
      "visibility feature contains: 2358 missing values. 0.02% of the dataset\n",
      "pressure feature contains: 4442 missing values. 0.03% of the dataset\n",
      "temperature feature contains: 858 missing values. 0.01% of the dataset\n",
      "dewpoint feature contains: 920 missing values. 0.01% of the dataset\n",
      "events feature contains: 1626 missing values. 0.01% of the dataset\n",
      "conditions feature contains: 0 missing values. 0.0% of the dataset\n",
      "\n",
      "['trip_id', 'windchill', 'precipitation', 'dewpoint', 'from_station_id', 'to_station_id'] features were droppped successfully\n",
      "\n",
      "\n",
      "Missing start LAT & LON fixed\n",
      "Missing end LAT & LON fixed\n",
      "\n",
      "\n",
      "Null values were dropped and indexes were reset successfully\n",
      "\n",
      "\n",
      "Gender feature encoded\n",
      "UserType feature encoded\n",
      "\n",
      "\n",
      "Processing Datatype conversion ...\n",
      "\n",
      "usertype column processed...\n",
      "gender column processed...\n",
      "dpcapacity_start column processed...\n",
      "dpcapacity_end column processed...\n",
      "tripduration column processed...\n",
      "humidity column processed...\n",
      "rain column processed...\n",
      "dpcapacity_start column processed...\n",
      "dpcapacity_end column processed...\n",
      "temperature column processed...\n",
      "pressure column processed...\n",
      "visibility column processed...\n",
      "wind_speed column processed...\n",
      "Int columns processed successfully\n",
      "\n",
      "from_station_name column processed...\n",
      "to_station_name column processed...\n",
      "events column processed...\n",
      "conditions column processed...\n",
      "Category columns processed successfully\n",
      "\n",
      "Processing Datetime features ...\n",
      "\n",
      "starttime column processed...\n",
      "stoptime column processed...\n",
      "Datetime columns processed successfully\n",
      "\n",
      "\n",
      "latitude_start column processed...\n",
      "longitude_start column processed...\n",
      "latitude_end column processed...\n",
      "longitude_end column processed...\n",
      "4 decimal points LAT & LON processed successfully\n",
      "\n",
      "[new_tripduration] feature created based on (stoptime - starttime) and the [tripduration] dropped\n",
      "Data with trip duration between 5 to 60 mins kept\n",
      "\n",
      "\n",
      "2013 and 2014 records dropped\n",
      "New dataset length is: 6680215 records. 7094500 records dropped\n",
      "\n",
      "\n",
      "Dropped values percentage: 51.5%\n",
      "DataFrame memory usage after optimization: 0.44 GB\n",
      "\n",
      "\n",
      "Processing time: 0:20:49.595770\n"
     ]
    }
   ],
   "source": [
    "df = func_process_datasets( path = 'Dataset/data_raw.csv', \n",
    "                            to_del = to_delete_cols, \n",
    "                            to_datetime = to_datetime_cols,\n",
    "                            to_int = to_int8_cols,\n",
    "                            to_cat = to_category_cols,\n",
    "                            lat_lon = to_round4_latlon )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('divvy_bikeshare.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations #1\n",
    "- latitude_start & longitude_start features have 1153 missing values. Since the LAT & LON are related to a specific station, we can simply get the station LAT & LON then assign its values into the missing ones. Similarly to the latitude_end & longitude_end as they have 1180 missing values. \n",
    "- Station: 'Clark St & 9th St (AMLI)' is the only one has missing values in latitude_start & longitude_start features. The values need to be assigned are 41.870815, -87.631248\n",
    "- Similarly I will repeat the above steps to identify the missing values in latitude_end & longitude_end and assign the missing values\n",
    "- Apperntly it's the same station 'Clark St & 9th St (AMLI)'. To double check that I will use the .value_counts() method\n",
    "\n",
    "### Observation #2\n",
    "\n",
    "- Gender feature contains -25 as value while it suppose to contain either 0 for Male and 1 for Female. Since it's for training purpose, I will take them off.\n",
    "- Test missing values\n",
    "    - Replace specific numbers into np.nan\n",
    "    - Compare the missing percentage with the length of the dataframe\n",
    "- Remove columns where values are missing \n",
    "- Drop columns where missing values are more than 80%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
